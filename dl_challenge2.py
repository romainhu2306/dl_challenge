# -*- coding: utf-8 -*-
"""dl_challenge2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OmsUQbXZ2QYnvW62s6eTGZIUka94IcbM
"""

!pip install pyarrow

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import holidays
from google.colab import drive

# Mount drive to access files directly from it.
drive.mount('/content/drive')

# Getting device.
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

"""## DATA MANAGEMENT"""

# Loading files.
train = pd.read_csv('/content/drive/MyDrive/Suricates/train.csv')
test = pd.read_csv('/content/drive/MyDrive/Suricates/test.csv')
meteo = pd.read_parquet('/content/drive/MyDrive/Suricates/meteo.parquet')
co2 = pd.read_csv('/content/drive/MyDrive/Suricates/co2_emissions.csv')

# Converting to datetime.
train['date'] = pd.to_datetime(train['date'], utc = True)
test['date'] = pd.to_datetime(test['date'], utc = True)
meteo['date'] = pd.to_datetime(meteo['date'], utc = True)

# Filling nan values for metropolises.
train['day_of_year'] = train['date'].dt.day_of_year
train['hour'] = train['date'].dt.hour

for col in train.columns:
  if col not in ['date', 'day_of_year', 'hour']:
    train[col] = train[col].fillna(train.groupby(['day_of_year', 'hour'])[col].transform('mean'))

train.drop(columns = ['day_of_year', 'hour'], inplace = True)

# Adding CO2 emissions per year.
co2.rename(columns = {'Year':'year', 'CO2 emissions from fuel combustion, France':'co2'}, inplace = True)
co2.drop(columns = 'Units', inplace = True)

scaler = StandardScaler()
co2['co2'] = scaler.fit_transform(co2[['co2']])

train['year'] = train['date'].dt.year
test['year'] = test['date'].dt.year

train = train.merge(co2, how = 'left', on = 'year')
test = test.merge(co2, how = 'left', on = 'year')

train.drop(columns = 'year', inplace = True)
test.drop(columns = 'year', inplace = True)

# Scaling wind speed.
scaler = StandardScaler()
meteo['ff'] = scaler.fit_transform(meteo[['ff']])

# We extract wind per region.
wind = meteo[['date', 'ff', 'numer_sta']]

# Resampling to half-hourly frequency
wind = pd.pivot_table(wind, values = 'ff', index = 'date', columns = 'numer_sta')
wind = wind.loc[:, wind.isnull().mean() <= .6]
wind = wind.resample('30min').interpolate(method = 'linear', limit_direction = 'both')

# Dividing train and test datasets.
train_wind = wind.loc['2017-02-13 00:30:00+00:00':'2021-12-31 22:30:00+00:00'].reset_index()
test_wind = wind.loc['2021-12-31 23:00:00+00:00':].reset_index()

# Merging wind.
train = train.merge(train_wind, how = 'left', on = ['date'])
test = test.merge(test_wind, how = 'left', on = ['date'])

# Scaling wind direction.
meteo['dd'] = np.sin(2*np.pi*meteo['dd']/360)

# We extract wind per region.
windd = meteo[['date', 'dd', 'numer_sta']]

# Resampling to half-hourly frequency
windd = pd.pivot_table(windd, values = 'dd', index = 'date', columns = 'numer_sta')
windd = wind.loc[:, wind.isnull().mean() <= .6]
windd = wind.resample('30min').interpolate(method = 'linear', limit_direction = 'both')

# Dividing train and test datasets.
train_windd = windd.loc['2017-02-13 00:30:00+00:00':'2021-12-31 22:30:00+00:00'].reset_index()
test_windd = windd.loc['2021-12-31 23:00:00+00:00':].reset_index()

# Merging wind.
train = train.merge(train_windd, how = 'left', on = ['date'], suffixes = ('_', '_d'))
test = test.merge(test_windd, how = 'left', on = ['date'], suffixes = ('_', '_d'))

# Scaling temperature.
scaler = StandardScaler()
meteo['t'] = scaler.fit_transform(meteo[['t']])

# We extract temperatures per region.
temp = meteo[['date', 't', 'numer_sta']]

# Resampling to half-hourly frequency
temp = pd.pivot_table(temp, values = 't', index = 'date', columns = 'numer_sta')
temp = temp.loc[:, temp.isnull().mean() <= .3]
temp = temp.resample('30min').interpolate(method = 'linear', limit_direction = 'both')
temp['t_mean'] = temp.mean(axis = 1)

# Divinding train and test temperature.
train_temp = temp.loc['2017-02-13 00:30:00+00:00':'2021-12-31 22:30:00+00:00'].reset_index()
test_temp = temp.loc['2021-12-31 23:00:00+00:00':].reset_index()

# Merging temperature.
train = train.merge(train_temp, how = 'left', on = ['date'])
test = test.merge(test_temp, how = 'left', on = ['date'])

# Scaling visibility.
scaler = StandardScaler()
meteo['vv'] = scaler.fit_transform(meteo[['vv']])

# We extract wind per region.
vis = meteo[['date', 'vv', 'numer_sta']]

# Resampling to half-hourly frequency
vis = pd.pivot_table(vis, values = 'vv', index = 'date', columns = 'numer_sta')
vis = wind.loc[:, vis.isnull().mean() <= .6]
vis = vis.resample('30min').interpolate(method = 'linear', limit_direction = 'both')

# Dividing train and test datasets.
train_vis = vis.loc['2017-02-13 00:30:00+00:00':'2021-12-31 22:30:00+00:00'].reset_index()
test_vis = vis.loc['2021-12-31 23:00:00+00:00':].reset_index()

# Merging wind.
train = train.merge(train_vis, how = 'left', on = ['date'])
test = test.merge(test_vis, how = 'left', on = ['date'])

# Scaling humidity.
scaler = StandardScaler()
meteo['u'] = scaler.fit_transform(meteo[['u']])

# We extract wind per region.
hum = meteo[['date', 'u', 'numer_sta']]

# Resampling to half-hourly frequency
hum = pd.pivot_table(hum, values = 'u', index = 'date', columns = 'numer_sta')
hum = wind.loc[:, hum.isnull().mean() <= .6]
hum = hum.resample('30min').interpolate(method = 'linear', limit_direction = 'both')

# Dividing train and test datasets.
train_hum = hum.loc['2017-02-13 00:30:00+00:00':'2021-12-31 22:30:00+00:00'].reset_index()
test_hum = hum.loc['2021-12-31 23:00:00+00:00':].reset_index()

# Merging wind.
train = train.merge(train_hum, how = 'left', on = ['date'], suffixes = ('_', '_z'))
test = test.merge(test_hum, how = 'left', on = ['date'], suffixes = ('_', '_z'))

# Scaling snow height.
scaler = StandardScaler()
meteo['ht_neige'] = scaler.fit_transform(meteo[['ff']])

# We extract wind per region.
snow = meteo[['date', 'ht_neige', 'numer_sta']]

# Resampling to half-hourly frequency
snow = pd.pivot_table(snow, values = 'ht_neige', index = 'date', columns = 'numer_sta')
snow = snow.loc[:, snow.isnull().mean() <= .6]
snow = snow.resample('30min').interpolate(method = 'linear', limit_direction = 'both')

# Dividing train and test datasets.
train_snow = wind.loc['2017-02-13 00:30:00+00:00':'2021-12-31 22:30:00+00:00'].reset_index()
test_snow = wind.loc['2021-12-31 23:00:00+00:00':].reset_index()

# Merging wind.
train = train.merge(train_snow, how = 'left', on = ['date'], suffixes = ('_', '_s'))
test = test.merge(test_snow, how = 'left', on = ['date'], suffixes = ('_', '_s'))

# Scaling rr12.
scaler = StandardScaler()
meteo['rr12'] = scaler.fit_transform(meteo[['rr12']])

# We extract wind per region.
rr12 = meteo[['date', 'rr12', 'numer_sta']]

# Resampling to half-hourly frequency
rr12 = pd.pivot_table(rr12, values = 'rr12', index = 'date', columns = 'numer_sta')
rr12 = rr12.loc[:, rr12.isnull().mean() <= .6]
rr12 = rr12.resample('30min').interpolate(method = 'linear', limit_direction = 'both')

# Dividing train and test datasets.
train_rr12 = rr12.loc['2017-02-13 00:30:00+00:00':'2021-12-31 22:30:00+00:00'].reset_index()
test_rr12 = rr12.loc['2021-12-31 23:00:00+00:00':].reset_index()

# Merging wind.
train = train.merge(train_rr12, how = 'left', on = ['date'], suffixes = ('_a', '_r'))
test = test.merge(test_rr12, how = 'left', on = ['date'], suffixes = ('_a', '_r'))

# Scaling nebulosity.
scaler = StandardScaler()
meteo['n'] = scaler.fit_transform(meteo[['n']])

# We extract wind per region.
neb = meteo[['date', 'n', 'numer_sta']]

# Resampling to half-hourly frequency
neb = pd.pivot_table(neb, values = 'n', index = 'date', columns = 'numer_sta')
neb = neb.loc[:, neb.isnull().mean() <= .6]
neb = neb.resample('30min').interpolate(method = 'linear', limit_direction = 'both')

# Dividing train and test datasets.
train_neb = neb.loc['2017-02-13 00:30:00+00:00':'2021-12-31 22:30:00+00:00'].reset_index()
test_neb = neb.loc['2021-12-31 23:00:00+00:00':].reset_index()

# Merging wind.
train = train.merge(train_neb, how = 'left', on = ['date'], suffixes = ('_b', '_n'))
test = test.merge(test_neb, how = 'left', on = ['date'], suffixes = ('_b', '_n'))

# Filling test set.
test.interpolate(method = 'linear', limit_direction = 'both', inplace = True)

# Extracting temporal features.
def extract_date(df, date_col = 'date'):
  df['day_of_year'] = np.sin(2*np.pi*df[date_col].dt.day_of_year/365)
  df['day_of_week'] = np.sin(2*np.pi*df[date_col].dt.day_of_week/7)
  df['hour'] = np.sin(2*np.pi*df[date_col].dt.hour/24)
  df['minute'] = np.sin(2*np.pi*df[date_col].dt.minute/60)
  return df

train = extract_date(train)
test = extract_date(test)

# Adding days-off.
fr_holidays = holidays.France()
train['day_off'] = train['date'].apply(lambda x: x in fr_holidays).astype(int)
test['day_off'] = test['date'].apply(lambda x: x in fr_holidays).astype(int)

# Adding covid confinement.
train['covid'] = (train['date'] >= '2020-03-17') & (train['date'] <= '2021-05-03')
test['covid'] = (test['date'] >= '2020-03-17') & (test['date'] <= '2021-05-03')
train['covid'] = train['covid'].astype(int)
test['covid'] = test['covid'].astype(int)

# Dropping original date column.
train.drop(columns = 'date', inplace = True)
test.drop(columns = 'date', inplace = True)

train.columns

X_train = torch.tensor(train.iloc[:55000, 25:].values, dtype = torch.float32)
y_train = torch.tensor(train.iloc[:55000, :25].values, dtype = torch.float32)
X_valid = torch.tensor(train.iloc[65000:, 25:].values, dtype = torch.float32)
y_valid = torch.tensor(train.iloc[65000:, :25].values, dtype = torch.float32)

X_test = torch.tensor(test.values, dtype = torch.float32)

train_set = torch.utils.data.TensorDataset(X_train, y_train)
train_loader = torch.utils.data.DataLoader(train_set, batch_size = 1024, shuffle = True)

"""## MODELS

### BASE MODEL
"""

class base(nn.Module):
  def __init__(self, input_dim, output_dim):
    super(base, self).__init__()
    self.L1 = nn.Linear(input_dim, 1024, bias = True)
    self.BN1 = nn.BatchNorm1d(num_features = 1024)
    self.drop1 = nn.Dropout(.2)
    self.act = nn.SiLU()
    self.L2 = nn.Linear(1024, 512, bias = True)
    self.BN2 = nn.BatchNorm1d(num_features = 512)
    self.drop2 = nn.Dropout(.2)
    self.L3 = nn.Linear(512, 128, bias = True)
    self.BN3 = nn.BatchNorm1d(num_features = 128)
    self.drop3 = nn.Dropout(.2)
    self.L4 = nn.Linear(128, output_dim, bias = True)

  def forward(self, x):
    x = self.L1(x)
    x = self.BN1(x)
    #x = self.drop1(x)
    x = self.act(x)
    x = self.L2(x)
    x = self.BN2(x)
    #x = self.drop2(x)
    x = self.act(x)
    x = self.L3(x)
    x = self.BN3(x)
    #x = self.drop3(x)
    x = self.act(x)
    x = self.L4(x)
    return x

"""#### TRAINING"""

input_dim = X_train.shape[1]
output_dim = y_train.shape[1]
model = base(input_dim, output_dim).to(device)

criterion = nn.MSELoss(reduction = 'mean')
optimizer = optim.Adam(model.parameters(), lr = .1)
#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', factor = .1, patience = 0)
num_epochs = 200

for epoch in range(num_epochs):
  model.train()
  ep_loss = .0
  for X, y in train_loader:
    X = X.to(device)
    y = y.to(device)
    optimizer.zero_grad()
    outputs = model(X)
    loss = criterion(outputs, y)
    loss.backward()
    optimizer.step()
    ep_loss += loss.item()
  ep_loss = np.sqrt(ep_loss/len(train_loader))
  print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {ep_loss:.4f}")

"""#### VALIDATION"""

nrows = len(X_valid)
table = torch.empty((nrows, output_dim)).to(device)
model.eval()
with torch.no_grad():
  for i in range(nrows):
    x = X_valid[i].unsqueeze(0).to(device)
    x = model(x)
    table[i] = x[0]
  y_valid = y_valid.to(device)
  loss = criterion(table, y_valid)
  print(np.sqrt(loss.item()))

plt.plot(np.arange(len(table)), (y_valid[:, 0] - table[:, 0]))
plt.plot(np.arange(len(table)), np.zeros(len(table)))
plt.show()

"""### SINE MODEL"""

class sine(nn.Module):
  def __init__(self, input_dim, output_dim):
    super(sine, self).__init__()
    self.L1 = nn.Linear(input_dim, 1024, bias = True)
    self.BN1 = nn.BatchNorm1d(num_features = 1024)
    self.drop1 = nn.Dropout(.2)
    self.act = nn.SiLU()
    self.L2 = nn.Linear(1024, 512, bias = True)
    self.BN2 = nn.BatchNorm1d(num_features = 512)
    self.drop2 = nn.Dropout(.2)
    self.L3 = nn.Linear(512, 128, bias = True)
    self.BN3 = nn.BatchNorm1d(num_features = 128)
    self.drop3 = nn.Dropout(.2)
    self.L4 = nn.Linear(128, output_dim, bias = True)

  def forward(self, x):
    x = self.L1(x)
    x = self.BN1(x)
    #x = self.drop1(x)
    x = torch.sin(x)
    x = self.L2(x)
    x = self.BN2(x)
    #x = self.drop2(x)
    x = self.act(x)
    x = self.L3(x)
    x = self.BN3(x)
    #x = self.drop3(x)
    x = self.act(x)
    x = self.L4(x)
    return x

"""#### TRAINING"""

input_dim = X_train.shape[1]
output_dim = y_train.shape[1]
model = sine(input_dim, output_dim).to(device)

criterion = nn.MSELoss(reduction = 'mean')
optimizer = optim.Adam(model.parameters(), lr = .1)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', factor = .1, patience = 0)
num_epochs = 70

for epoch in range(num_epochs):
  model.train()
  ep_loss = .0
  for X, y in train_loader:
    X = X.to(device)
    y = y.to(device)
    optimizer.zero_grad()
    outputs = model(X)
    loss = criterion(outputs, y)
    loss.backward()
    optimizer.step()
    ep_loss += loss.item()
  ep_loss = np.sqrt(ep_loss/len(train_loader))
  print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {ep_loss:.4f}")

"""#### VALIDATION"""

nrows = len(X_valid)
table = torch.empty((nrows, output_dim)).to(device)
model.eval()
with torch.no_grad():
  for i in range(nrows):
    x = X_valid[i].unsqueeze(0).to(device)
    x = model(x)
    table[i] = x[0]
  y_valid = y_valid.to(device)
  loss = criterion(table, y_valid)
  print(np.sqrt(loss.item()))

plt.plot(np.arange(len(table)), (y_valid[:, 0] - table[:, 0]))
plt.plot(np.arange(len(table)), np.zeros(len(table)))
plt.show()

"""### AGGREGATED MODEL"""

class net(nn.Module):
  def __init__(self, input_dim, hidden_dim, output_dim):
    super(net, self).__init__()
    self.L1 = nn.Linear(input_dim, hidden_dim, bias = True)
    self.BN = nn.BatchNorm1d(num_features = hidden_dim)
    self.act = nn.ReLU()
    self.L2 = nn.Linear(hidden_dim, output_dim, bias = True)

  def forward(self, x):
    x = self.L1(x)
    x = self.BN(x)
    x = self.act(x)
    x = self.L2(x)
    return x

def OL(model1, model2, alpha1, alpha2, eps = 1e-8):
  params1 = torch.cat([p.view(-1) for p in model1.parameters()])
  params2 = torch.cat([p.view(-1) for p in model2.parameters()])

  dot = torch.dot(params1, params2)
  norm1 = torch.norm(params1)
  norm2 = torch.norm(params2)

  loss = alpha1*torch.abs(dot) + alpha2/(norm1 + eps) + alpha2/(norm2 + eps)
  return loss

"""#### TRAINING"""

input_dim = X_train.shape[1]
output_dim = y_train.shape[1]
alpha1 = 1
alpha2 = 1
dotloss = []
mseloss = []

mod1 = net(input_dim, 512, output_dim).to(device)
mod2 = net(input_dim, 512, output_dim).to(device)
mod3 = net(2*output_dim, 512, output_dim).to(device)

mse = nn.MSELoss(reduction = 'mean')

opti1 = optim.Adam(mod1.parameters(), lr = .001)
opti2 = optim.Adam(mod2.parameters(), lr = .001)
opti3 = optim.Adam(mod3.parameters(), lr = .1)

num_epochs = 100

for epoch in range(num_epochs):
  mod1.train()
  mod2.train()
  mod3.train()
  ep_loss = .0

  for X, y in train_loader:
    X = X.to(device)
    y = y.to(device)

    opti1.zero_grad()
    opti2.zero_grad()
    opti3.zero_grad()

    out1 = mod1(X)
    out2 = mod2(X)
    out12 = torch.cat((out1, out2), dim = 1)
    out3 = mod3(out12)

    loss2 = OL(mod1, mod2, alpha1, alpha2)
    loss1 = mse(out3, y)
    loss = loss1 + loss2
    loss.backward()
    opti3.step()
    opti2.step()
    opti1.step()

    ep_loss += loss1.item()
    dotloss.append(loss2.item())
    mseloss.append(loss1.item())
  ep_loss = np.sqrt(ep_loss/len(train_loader))
  print(f"Epoch {epoch + 1}/{num_epochs}, MSE Loss: {ep_loss:.4f}, Dot loss: {loss2: .4f}")

plt.plot(np.arange(0, len(dotloss)), dotloss)
plt.show()

"""#### VALIDATION"""

nrows = len(X_valid)
table = torch.empty((nrows, output_dim)).to(device)
mod1.eval()
mod2.eval()
mod3.eval()
with torch.no_grad():
  for i in range(nrows):
    x = X_valid[i].unsqueeze(0).to(device)
    x1 = mod1(x)
    x2 = mod2(x)
    x12 = torch.cat((x1, x2), dim = 1)
    x3 = mod3(x12)
    table[i] = x3[0]
  y_valid = y_valid.to(device)
  loss = mse(table, y_valid)
  print(np.sqrt(loss.item()))

plt.plot(np.arange(len(table)), (y_valid[:, 0] - table[:, 0]))
plt.plot(np.arange(len(table)), np.zeros(len(table)))
plt.show()

nrows = len(X_valid)
table1 = torch.empty((nrows, output_dim)).to(device)
table2 = torch.empty((nrows, output_dim)).to(device)
mod1.eval()
mod2.eval()
with torch.no_grad():
  for i in range(nrows):
    x = X_valid[i].unsqueeze(0).to(device)
    x1 = mod1(x)
    x2 = mod2(x)
    table1[i] = x1[0]
    table2[i] = x2[0]
  y_valid = y_valid.to(device)
  loss1 = mse(table1, y_valid)
  loss2 = mse(table2, y_valid)
  print(np.sqrt(loss1.item()), np.sqrt(loss2.item()))

plt.plot(np.arange(len(table)), table1[:, 8])
plt.plot(np.arange(len(table)), table2[:, 8])
plt.plot(np.arange(len(table)), np.zeros(len(table)))
plt.show()

"""### BASE AND SINE AGGREGATION

#### TRAINING
"""

input_dim = X_train.shape[1]
output_dim = y_train.shape[1]
alpha1 = 1
alpha2 = 1
dotloss = []
mseloss = []

mod1 = base(input_dim, output_dim).to(device)
mod2 = sine(input_dim, output_dim).to(device)
mod3 = net(2*output_dim, 512, output_dim).to(device)

mse = nn.MSELoss(reduction = 'mean')

opti1 = optim.Adam(mod1.parameters(), lr = .01)
opti2 = optim.Adam(mod2.parameters(), lr = .01)
opti3 = optim.Adam(mod3.parameters(), lr = .01)

num_epochs = 100

for epoch in range(num_epochs):
  mod1.train()
  mod2.train()
  mod3.train()
  ep_loss = .0

  for X, y in train_loader:
    X = X.to(device)
    y = y.to(device)

    opti1.zero_grad()
    opti2.zero_grad()
    opti3.zero_grad()

    out1 = mod1(X)
    out2 = mod2(X)
    out12 = torch.cat((out1, out2), dim = 1)
    out3 = mod3(out12)

    loss = mse(out3, y)
    loss.backward()
    opti3.step()
    opti2.step()
    opti1.step()

    ep_loss += loss.item()
  ep_loss = np.sqrt(ep_loss/len(train_loader))
  print(f"Epoch {epoch + 1}/{num_epochs}, MSE Loss: {ep_loss:.4f}")

"""#### VALIDATION"""

nrows = len(X_valid)
table = torch.empty((nrows, output_dim)).to(device)
mod1.eval()
mod2.eval()
mod3.eval()
with torch.no_grad():
  for i in range(nrows):
    x = X_valid[i].unsqueeze(0).to(device)
    x1 = mod1(x)
    x2 = mod2(x)
    x12 = torch.cat((x1, x2), dim = 1)
    x3 = mod3(x12)
    table[i] = x3[0]
  y_valid = y_valid.to(device)
  loss = mse(table, y_valid)
  print(np.sqrt(loss.item()))

plt.plot(np.arange(len(table)), (y_valid[:, 0] - table[:, 0]))
plt.plot(np.arange(len(table)), np.zeros(len(table)))
plt.show()

"""### AGGREGATED DUEL MODEL"""

class net(nn.Module):
  def __init__(self, input_dim, hidden_dim, output_dim):
    super(net, self).__init__()
    self.L1 = nn.Linear(input_dim, hidden_dim, bias = True)
    self.BN = nn.BatchNorm1d(num_features = hidden_dim)
    self.act = nn.ReLU()
    self.L2 = nn.Linear(hidden_dim, output_dim, bias = True)

  def forward(self, x):
    x = self.L1(x)
    x = self.BN(x)
    x = self.act(x)
    x = self.L2(x)
    return x


class aggreg(nn.Module):
  def __init__(self, a1, a2):
    super(aggreg, self).__init__()
    self.raw_coefs = nn.Parameter(torch.tensor([a1, a2]), requires_grad = True)

  def forward(self, out1, out2):
    coefs = torch.softmax(self.raw_coefs, dim = 0)
    x = coefs[0]*out1 + coefs[1]*out2
    return x, coefs

"""#### TRAINING"""

input_dim = X_train.shape[1]
output_dim = y_train.shape[1]
target_weight = torch.tensor(1, dtype = torch.float32)
a1 = .9
a2 = .1

coefs_list1 = []
coefs_list2 = []

mod1 = net(input_dim, 512, output_dim).to(device)
mod2 = net(input_dim, 512, output_dim).to(device)
mod4 = aggreg(a1, a2).to(device)

mse = nn.MSELoss(reduction = 'mean')
mae = nn.L1Loss(reduction = 'mean')

opti1 = optim.Adam(mod1.parameters(), lr = .001)
opti2 = optim.Adam(mod2.parameters(), lr = .001)
opti4 = optim.Adam(mod4.parameters(), lr = .1)

num_epochs = 1

for epoch in range(num_epochs):
  mod1.train()
  mod2.train()
  mod4.train()
  ep_loss = .0

  for X, y in train_loader:
    X = X.to(device)
    y = y.to(device)

    opti1.zero_grad()
    opti2.zero_grad()
    opti4.zero_grad()

    out1 = mod1(X)
    out2 = mod2(X)
    out4 = mod4(out1.detach(), out2.detach())
    output = out4[0]
    coefs = out4[1]

    loss2 = mae(coefs[1], target_weight)
    loss2.backward(retain_graph = True)
    opti2.step()

    loss1 = mae(coefs[0], target_weight)
    loss1.backward(retain_graph = True)
    opti1.step()

    loss4 = mse(output, y)
    loss4.backward()
    opti4.step()

    coefs_list1.append(coefs[0].detach().numpy())
    coefs_list2.append(coefs[1].detach().numpy())
    print(f"Loss: {np.sqrt(loss4.item()):.4f}, Coefs: {coefs[0]:.4f}, {coefs[1]:.4f}")

"""#### TEST"""

plt.plot(np.arange(0, len(coefs_list1)), coefs_list1)
plt.plot(np.arange(0, len(coefs_list2)), coefs_list2)
plt.show()

nrows = len(X_valid)
table1 = torch.empty((nrows, output_dim)).to(device)
table2 = torch.empty((nrows, output_dim)).to(device)
mod1.eval()
mod2.eval()

with torch.no_grad():
  for i in range(nrows):
    x = X_valid[i].unsqueeze(0).to(device)
    out1 = mod1(x)
    out2 = mod2(x)
    table1[i] = out1[0]
    table2[i] = out2[0]

  y_valid = y_valid.to(device)
  loss1 = mse(table1, y_valid)
  loss2 = mse(table2, y_valid)
  print(np.sqrt(loss1.item()), np.sqrt(loss2.item()))

plt.plot(np.arange(len(table1)), table1[:, 0])
plt.plot(np.arange(len(table1)), table2[:, 0])
plt.plot(np.arange(len(table1)), np.zeros(len(table1)))
plt.show()

"""#### PREDICTION"""

pred = pd.read_csv('/content/drive/MyDrive/Suricates/template.csv')
nrows = len(pred)
model.eval()
with torch.no_grad():
  for i in range(nrows):
    x = X_test[i].unsqueeze(0).to(device)
    y = model(x)
    pred.iloc[i, 1:] = y[0].numpy()

pred

plt.plot(pred.index[:], pred.iloc[:, 1])

ref = pd.read_csv('/content/drive/MyDrive/Suricates/pred.csv')
ref.iloc[:, 1:] - pred.iloc[:, 1:]

plt.plot(pred.index, (ref.iloc[:, 1:] - pred.iloc[:, 1:]).sum(axis = 1))
plt.plot(pred.index, np.zeros(len(pred.index)))

pred.set_index('date', inplace = True)
pred.to_csv('/content/drive/MyDrive/Suricates/pred_wait.csv')